{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"/home/richard/labrotation/processed_sample_data/human_origins_labels.csv\"\n",
    "array_path = \"//home/richard/labrotation/processed_sample_data/arrays\"\n",
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_csv(csv_path)\n",
    "array_dir = array_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sampleDataset(Dataset):\n",
    "    def __init__(self, csv_file, array_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.array_dir = array_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        array_path = os.path.join(self.array_dir, self.annotations.iloc[index, 0]+\".npy\")\n",
    "        array = np.load(array_path)\n",
    "        array = array.astype('float')\n",
    "        y_label = annotations.iloc[index,1]\n",
    "\n",
    "        if self.transform:\n",
    "            array = self.transform(array)\n",
    "        \n",
    "        return array, y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sampleDataset(csv_file=csv_path, array_dir=array_dir)\n",
    "data_loader = DataLoader(dataset=dataset, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 1000])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch = next(iter(data_loader))\n",
    "test_array = test_batch[0]\n",
    "test_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_array.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "expected input: Tensor of dimensions N*C*in_features\n",
    "output: Tensor of dimensions N*out_sets*⌈(in_features/m)⌉\n",
    "N: batch size\n",
    "C: number of channels (e.g. 4 for one-hot-encoding of SNPs)\n",
    "in_features: number of input features (e.g. SNP positions)\n",
    "out_sets: number of output sets (new channels)\n",
    "m: how many in_features to group together\n",
    "kernel_size: kernel of flat tensor: m*C\n",
    "padding: should we padd at the end of the dataframe if in_features%m !=0? \n",
    "'''\n",
    "class LocallyConnectedLayer(torch.nn.Module):\n",
    "    def __init__(self, in_features, m, C=4, padding=True, bias=False, out_sets=4):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.C = C\n",
    "        self.m = m\n",
    "        self.kernel_size = m*C\n",
    "        self.padding = (m-(in_features%m))%m*C if padding else 0\n",
    "        self.weight = nn.Parameter(torch.randn(1,self.kernel_size, out_sets))\n",
    "        self.bias = nn.Parameter(torch.randn(1,out_sets)) if bias else None # first try without bias but with batchnorm\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = x.transpose(-1,-2) # we need to transpose first to ensure that the channel values of one in_feature are next to each other after flattening\n",
    "        x = x.flatten(1) # dim(N,in_features*C)\n",
    "        x = F.pad(x,(0,self.padding))\n",
    "        x = x.unfold(-1, size=self.kernel_size, step=self.kernel_size)\n",
    "        x = torch.matmul(x.float(),self.weight)\n",
    "        if self.bias is not None:\n",
    "            x = x+self.bias\n",
    "        x = x = x.transpose(-1,-2) # transpose back to have the more convenient dimension order\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LCBlock(nn.Module):\n",
    "    def __init__(self, in_features, m, out_sets=4, p=0.0):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm1d(out_sets)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.LCL1 = LocallyConnectedLayer(in_features, m=m, padding=True, out_sets=out_sets)\n",
    "        self.LCL2 = LocallyConnectedLayer(in_features=math.ceil(in_features/m),m=m, padding=True, out_sets=out_sets)\n",
    "        self.identity_downsample = nn.Linear(in_features, out_features=math.ceil(in_features/m**2)) if m!=1 else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        x = self.bn(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.LCL1(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.LCL2(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "\n",
    "        x = x+identity\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "expected input: flat tensor of shape N*in_features\n",
    "expected output: flat tensor of shape N*out_features\n",
    "N: batch size\n",
    "'''\n",
    "class FCBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, p=0.5):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm1d(in_features)\n",
    "        self.bn2 = nn.BatchNorm1d(out_features)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.FCL1 = nn.Linear(in_features=in_features, out_features=out_features)\n",
    "        self.FCL2 = nn.Linear(in_features=out_features, out_features=out_features)\n",
    "        self.identity_downsample = nn.Linear(in_features, out_features=out_features) if in_features != out_features else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        x = self.bn1(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.FCL1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.FCL2(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "\n",
    "        x = x+identity\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLN(nn.Module):\n",
    "    def __init__(self, block, num_residual_blocks, in_features):\n",
    "        self.LCL0 = LocallyConnectedLayer(in_features, m=2)\n",
    "        self.block1\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.LCL0(in_features, m=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCBlock(in_features=4000, out_features=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Double but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/richard/labrotation/Lab-Rotation/scripts/data_loader.ipynb Cell 13'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/richard/labrotation/Lab-Rotation/scripts/data_loader.ipynb#ch0000015vscode-remote?line=0'>1</a>\u001b[0m test_array\u001b[39m.\u001b[39mdtype\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/richard/labrotation/Lab-Rotation/scripts/data_loader.ipynb#ch0000015vscode-remote?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39;49mforward(test_array\u001b[39m.\u001b[39;49mflatten(\u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39mshape\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/richard/labrotation/Lab-Rotation/scripts/data_loader.ipynb#ch0000015vscode-remote?line=2'>3</a>\u001b[0m bla\u001b[39m.\u001b[39mforward(test_array)\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;32m/home/richard/labrotation/Lab-Rotation/scripts/data_loader.ipynb Cell 10'\u001b[0m in \u001b[0;36mFCBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/richard/labrotation/Lab-Rotation/scripts/data_loader.ipynb#ch0000008vscode-remote?line=16'>17</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/richard/labrotation/Lab-Rotation/scripts/data_loader.ipynb#ch0000008vscode-remote?line=17'>18</a>\u001b[0m     identity \u001b[39m=\u001b[39m x\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/richard/labrotation/Lab-Rotation/scripts/data_loader.ipynb#ch0000008vscode-remote?line=19'>20</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn1(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/richard/labrotation/Lab-Rotation/scripts/data_loader.ipynb#ch0000008vscode-remote?line=20'>21</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msilu(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/richard/labrotation/Lab-Rotation/scripts/data_loader.ipynb#ch0000008vscode-remote?line=21'>22</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mFCL1(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=160'>161</a>\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=162'>163</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=163'>164</a>\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=164'>165</a>\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=165'>166</a>\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=166'>167</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=167'>168</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=168'>169</a>\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=169'>170</a>\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=170'>171</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=171'>172</a>\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=172'>173</a>\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=173'>174</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=174'>175</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=175'>176</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=176'>177</a>\u001b[0m     bn_training,\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=177'>178</a>\u001b[0m     exponential_average_factor,\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=178'>179</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=179'>180</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:2282\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/functional.py?line=2278'>2279</a>\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/functional.py?line=2279'>2280</a>\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/functional.py?line=2281'>2282</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/functional.py?line=2282'>2283</a>\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/functional.py?line=2283'>2284</a>\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Double but found Float"
     ]
    }
   ],
   "source": [
    "test_array.dtype\n",
    "model.forward(test_array.flatten(1)).shape\n",
    "bla.forward(test_array).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "fc0 = LocallyConnectedLayer(in_features=1000,m=2,bias=False)\n",
    "a = torch.from_numpy(array).unsqueeze(0).float()\n",
    "out = fc0.forward(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = LocallyConnectedLayer(in_features=500, m=8).forward(a)\n",
    "c = LocallyConnectedLayer(in_features=125,m=8).forward(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bla = c.flatten()\n",
    "bla = bla.unsqueeze(0)\n",
    "bla2 = nn.BatchNorm1d(64)\n",
    "bla2(bla)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
