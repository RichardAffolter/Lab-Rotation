{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"/home/richard/labrotation/processed_sample_data/human_origins_labels.csv\"\n",
    "array_path = \"//home/richard/labrotation/processed_sample_data/arrays\"\n",
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sampleDataset(Dataset):\n",
    "    def __init__(self, csv_file, array_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.array_dir = array_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        array_path = os.path.join(self.array_dir, self.annotations.iloc[index, 0]+\".npy\")\n",
    "        array = np.load(array_path)\n",
    "        array = array.astype('float32')\n",
    "        y_label = annotations.iloc[index,1]\n",
    "\n",
    "        if self.transform:\n",
    "            y_label = self.transform((y_label,))\n",
    "        \n",
    "        return array, y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_csv(csv_path)\n",
    "array_dir = array_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = annotations['Origin'].unique()\n",
    "le = LabelEncoder()\n",
    "le.fit(classes)\n",
    "dataset = sampleDataset(csv_file=csv_path, array_dir=array_dir, transform=le.transform)\n",
    "data_loader = DataLoader(dataset=dataset, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tensor.flatten>"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch = next(iter(data_loader))\n",
    "test_array = test_batch[0]\n",
    "test_batch[1].flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "expected input: Tensor of dimensions N*C*in_features\n",
    "output: Tensor of dimensions N*out_sets*⌈(in_features/m)⌉\n",
    "N: batch size\n",
    "C: number of channels (e.g. 4 for one-hot-encoding of SNPs)\n",
    "in_features: number of input features (e.g. SNP positions)\n",
    "out_sets: number of output sets (new channels)\n",
    "m: how many in_features to group together\n",
    "kernel_size: kernel of flat tensor: m*C\n",
    "padding: should we padd at the end of the dataframe if in_features%m !=0? \n",
    "'''\n",
    "class LocallyConnectedLayer(torch.nn.Module):\n",
    "    def __init__(self, in_features, m, C=4, padding=True, bias=False, out_sets=4):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.C = C\n",
    "        self.m = m\n",
    "        self.kernel_size = m*C\n",
    "        self.padding = (m-(in_features%m))%m*C if padding else 0\n",
    "        self.weight = nn.Parameter(torch.randn(1,self.kernel_size, out_sets))\n",
    "        self.bias = nn.Parameter(torch.randn(1,out_sets)) if bias else None # with batchnorm we do not need bias\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(-1,-2) # we need to transpose first to ensure that the channel values of one in_feature are next to each other after flattening\n",
    "        x = x.flatten(1) # dim(N,in_features*C)\n",
    "        x = F.pad(x,(0,self.padding))\n",
    "        x = x.unfold(-1, size=self.kernel_size, step=self.kernel_size)\n",
    "        x = torch.matmul(x,self.weight)\n",
    "        if self.bias is not None:\n",
    "            x = x+self.bias\n",
    "        x = x = x.transpose(-1,-2) # transpose back to have the more convenient dimension order\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LCBlock(nn.Module):\n",
    "    def __init__(self, in_features, m, out_sets=4, p=0.0):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm1d(out_sets)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.LCL1 = LocallyConnectedLayer(in_features, m=m, padding=True, out_sets=out_sets)\n",
    "        self.LCL2 = LocallyConnectedLayer(in_features=math.ceil(in_features/m),m=m, padding=True, out_sets=out_sets)\n",
    "        self.identity_downsample = nn.Linear(in_features, out_features=math.ceil(in_features/m**2)) if m!=1 else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        x = self.bn(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.LCL1(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.LCL2(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "\n",
    "        x = x+identity\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "expected input: flat tensor of shape N*in_features\n",
    "expected output: flat tensor of shape N*out_features\n",
    "N: batch size\n",
    "'''\n",
    "class FCBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, p=0.5):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm1d(in_features)\n",
    "        self.bn2 = nn.BatchNorm1d(out_features)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.FCL1 = nn.Linear(in_features=in_features, out_features=out_features)\n",
    "        self.FCL2 = nn.Linear(in_features=out_features, out_features=out_features)\n",
    "        self.identity_downsample = nn.Linear(in_features, out_features=out_features) if in_features != out_features else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        x = self.bn1(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.FCL1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.FCL2(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "\n",
    "        x = x+identity\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLN(nn.Module):\n",
    "    def __init__(self, in_features, num_classes, num_residual_blocks=2, m1=2, m2=2, C=4, num_predictor_blocks=4):\n",
    "        super().__init__()\n",
    "        self.m1 = m1\n",
    "        self.m2 = m2\n",
    "        self.num_residual_blocks = num_residual_blocks\n",
    "        self.num_predictor_blocks = num_residual_blocks\n",
    "        self.LCL0 = LocallyConnectedLayer(in_features, m=m1)\n",
    "        Output1 = math.ceil(in_features/m1)\n",
    "        self.LCLayers = self.make_LCLayers(Output1)\n",
    "        Output2 = math.ceil(Output1/(2*m2)**num_residual_blocks)*C # we flatten after the last block TO DO: IMPLEMENT ENVIRONMENT CONCATENATION\n",
    "        self.FCLayers = self.make_predictorLayers(in_features=Output2)\n",
    "        self.bn = nn.BatchNorm1d(256)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        self.Linear = nn.Linear(256,num_classes)\n",
    "        \n",
    "\n",
    "    def make_LCLayers(self, in_features):\n",
    "        layers = []\n",
    "        for block in range(self.num_residual_blocks):\n",
    "            layers.append(LCBlock(in_features=in_features, m=self.m2))\n",
    "            in_features = math.ceil(in_features/self.m2**2)\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def make_predictorLayers(self, in_features):\n",
    "        layers = []\n",
    "        layers.append(FCBlock(in_features=in_features, out_features=256))\n",
    "        for block in range(self.num_predictor_blocks):\n",
    "            layers.append(FCBlock(in_features=256, out_features=256))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.LCL0(x)\n",
    "        x = self.LCLayers(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.FCLayers(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.Linear(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-4\n",
    "batch_size = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GLN(in_features=1000, num_classes=6,num_residual_blocks=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data,targets) in enumerate(data_loader):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.flatten().to(device=device)\n",
    "\n",
    "        # forwards\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # backwards\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent\n",
    "        optimizer.step()\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 2, 9, 7, 1, 4, 0, 2, 6])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.randint(0, 10, (10,)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
