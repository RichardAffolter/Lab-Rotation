{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"/home/richard/labrotation/processed_sample_data/human_origins_labels.csv\"\n",
    "array_path = \"//home/richard/labrotation/processed_sample_data/arrays\"\n",
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sampleDataset(Dataset):\n",
    "    def __init__(self, csv_file, array_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.array_dir = array_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        array_path = os.path.join(self.array_dir, self.annotations.iloc[index, 0]+\".npy\")\n",
    "        array = np.load(array_path)\n",
    "        array = array.astype('float32')\n",
    "        y_label = annotations.iloc[index,1]\n",
    "\n",
    "        if self.transform:\n",
    "            y_label = self.transform(y_label)\n",
    "        \n",
    "        return array, y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_csv(csv_path)\n",
    "array_dir = array_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation(x):\n",
    "    x = le.transform((x,))\n",
    "    x = x[0]\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = annotations['Origin'].unique()\n",
    "le = LabelEncoder()\n",
    "le.fit(classes)\n",
    "dataset = sampleDataset(csv_file=csv_path, array_dir=array_dir, transform=transformation)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=5, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=5, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 2, 0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch = next(iter(test_loader))\n",
    "test_array = test_batch[0]\n",
    "test_batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "expected input: Tensor of dimensions N*C*in_features\n",
    "output: Tensor of dimensions N*out_sets*⌈(in_features/m)⌉\n",
    "N: batch size\n",
    "C: number of channels (e.g. 4 for one-hot-encoding of SNPs)\n",
    "in_features: number of input features (e.g. SNP positions)\n",
    "out_sets: number of output sets (new channels)\n",
    "m: how many in_features to group together\n",
    "kernel_size: kernel of flat tensor: m*C\n",
    "padding: should we padd at the end of the dataframe if in_features%m !=0? \n",
    "'''\n",
    "class LocallyConnectedLayer(torch.nn.Module):\n",
    "    def __init__(self, in_features, m, C=4, padding=True, bias=False, out_sets=4):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.C = C\n",
    "        self.m = m\n",
    "        self.kernel_size = m*C\n",
    "        self.padding = (m-(in_features%m))%m*C if padding else 0\n",
    "        self.weight = nn.Parameter(torch.randn(1,self.kernel_size, out_sets))\n",
    "        self.bias = nn.Parameter(torch.randn(1,out_sets)) if bias else None # with batchnorm we do not need bias\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(-1,-2) # we need to transpose first to ensure that the channel values of one in_feature are next to each other after flattening\n",
    "        x = x.flatten(1) # dim(N,in_features*C)\n",
    "        x = F.pad(x,(0,self.padding))\n",
    "        x = x.unfold(-1, size=self.kernel_size, step=self.kernel_size)\n",
    "        x = torch.matmul(x,self.weight)\n",
    "        if self.bias is not None:\n",
    "            x = x+self.bias\n",
    "        x = x = x.transpose(-1,-2) # transpose back to have the more convenient dimension order\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LCBlock(nn.Module):\n",
    "    def __init__(self, in_features, m, out_sets=4, p=0.0):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm1d(out_sets)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.LCL1 = LocallyConnectedLayer(in_features, m=m, padding=True, out_sets=out_sets)\n",
    "        self.LCL2 = LocallyConnectedLayer(in_features=math.ceil(in_features/m),m=m, padding=True, out_sets=out_sets)\n",
    "        self.identity_downsample = nn.Linear(in_features, out_features=math.ceil(in_features/m**2)) if m!=1 else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        x = self.bn(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.LCL1(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.LCL2(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "\n",
    "        x = x+identity\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "expected input: flat tensor of shape N*in_features\n",
    "expected output: flat tensor of shape N*out_features\n",
    "N: batch size\n",
    "'''\n",
    "class FCBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, p=0.5):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm1d(in_features)\n",
    "        self.bn2 = nn.BatchNorm1d(out_features)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.FCL1 = nn.Linear(in_features=in_features, out_features=out_features)\n",
    "        self.FCL2 = nn.Linear(in_features=out_features, out_features=out_features)\n",
    "        self.identity_downsample = nn.Linear(in_features, out_features=out_features) if in_features != out_features else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        x = self.bn1(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.FCL1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.FCL2(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "\n",
    "        x = x+identity\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLN(nn.Module):\n",
    "    def __init__(self, in_features, num_classes, num_residual_blocks=2, m1=2, m2=2, C=4, num_predictor_blocks=4):\n",
    "        super().__init__()\n",
    "        self.m1 = m1\n",
    "        self.m2 = m2\n",
    "        self.num_residual_blocks = num_residual_blocks\n",
    "        self.num_predictor_blocks = num_residual_blocks\n",
    "        self.LCL0 = LocallyConnectedLayer(in_features, m=m1)\n",
    "        Output1 = math.ceil(in_features/m1)\n",
    "        self.LCLayers = self.make_LCLayers(Output1)\n",
    "        Output2 = math.ceil(Output1/(2*m2)**num_residual_blocks)*C # we flatten after the last block TO DO: IMPLEMENT ENVIRONMENT CONCATENATION\n",
    "        self.FCLayers = self.make_predictorLayers(in_features=Output2)\n",
    "        self.bn = nn.BatchNorm1d(256)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        self.Linear = nn.Linear(256,num_classes)\n",
    "        \n",
    "\n",
    "    def make_LCLayers(self, in_features):\n",
    "        layers = []\n",
    "        for block in range(self.num_residual_blocks):\n",
    "            layers.append(LCBlock(in_features=in_features, m=self.m2))\n",
    "            in_features = math.ceil(in_features/self.m2**2)\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def make_predictorLayers(self, in_features):\n",
    "        layers = []\n",
    "        layers.append(FCBlock(in_features=in_features, out_features=256))\n",
    "        for block in range(self.num_predictor_blocks):\n",
    "            layers.append(FCBlock(in_features=256, out_features=256))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.LCL0(x)\n",
    "        x = self.LCLayers(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.FCLayers(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.Linear(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-4\n",
    "batch_size = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            scores = model(x)\n",
    "            predictions = torch.argmax(scores, dim=1)\n",
    "            \n",
    "\n",
    "            num_correct += (predictions==y).sum()\n",
    "            num_samples += predictions.shape[0]\n",
    "        #acc = float(num_correct/num_samples)*100\n",
    "        #print(f' Got {num_correct} / {num_samples} with accuracy {float(num_correct/num_samples)*100}')\n",
    "\n",
    "    model.train()\n",
    "    return(num_correct,num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GLN(in_features=1000, num_classes=6,num_residual_blocks=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 0 was 1.48890\n",
      " Evaluation with training data got 2402 / 3770 with accuracy 63.71353\n",
      " Evaluation at testing data got 1010 / 1615 with accuracy 62.53870\n",
      "loss at epoch 5 was 0.64230\n",
      " Evaluation with training data got 3079 / 3770 with accuracy 81.67109\n",
      " Evaluation at testing data got 1203 / 1615 with accuracy 74.48916\n",
      "loss at epoch 10 was 0.42667\n",
      " Evaluation with training data got 3539 / 3770 with accuracy 93.87268\n",
      " Evaluation at testing data got 1273 / 1615 with accuracy 78.82353\n",
      "loss at epoch 15 was 0.23138\n",
      " Evaluation with training data got 3674 / 3770 with accuracy 97.45358\n",
      " Evaluation at testing data got 1271 / 1615 with accuracy 78.69969\n",
      "loss at epoch 20 was 0.17150\n",
      " Evaluation with training data got 3727 / 3770 with accuracy 98.85942\n",
      " Evaluation at testing data got 1273 / 1615 with accuracy 78.82353\n",
      "loss at epoch 25 was 0.10838\n",
      " Evaluation with training data got 3749 / 3770 with accuracy 99.44297\n",
      " Evaluation at testing data got 1275 / 1615 with accuracy 78.94737\n",
      "loss at epoch 30 was 0.05842\n",
      " Evaluation with training data got 3761 / 3770 with accuracy 99.76127\n",
      " Evaluation at testing data got 1284 / 1615 with accuracy 79.50464\n",
      "loss at epoch 35 was 0.04890\n",
      " Evaluation with training data got 3741 / 3770 with accuracy 99.23077\n",
      " Evaluation at testing data got 1297 / 1615 with accuracy 80.30960\n",
      "loss at epoch 40 was 0.03808\n",
      " Evaluation with training data got 3759 / 3770 with accuracy 99.70822\n",
      " Evaluation at testing data got 1301 / 1615 with accuracy 80.55727\n",
      "loss at epoch 45 was 0.03056\n",
      " Evaluation with training data got 3766 / 3770 with accuracy 99.89390\n",
      " Evaluation at testing data got 1296 / 1615 with accuracy 80.24768\n",
      "loss at epoch 50 was 0.03427\n",
      " Evaluation with training data got 3767 / 3770 with accuracy 99.92042\n",
      " Evaluation at testing data got 1294 / 1615 with accuracy 80.12384\n",
      "loss at epoch 55 was 0.02094\n",
      " Evaluation with training data got 3756 / 3770 with accuracy 99.62865\n",
      " Evaluation at testing data got 1279 / 1615 with accuracy 79.19505\n",
      "loss at epoch 60 was 0.02851\n",
      " Evaluation with training data got 3767 / 3770 with accuracy 99.92042\n",
      " Evaluation at testing data got 1287 / 1615 with accuracy 79.69040\n",
      "loss at epoch 65 was 0.01795\n",
      " Evaluation with training data got 3762 / 3770 with accuracy 99.78780\n",
      " Evaluation at testing data got 1280 / 1615 with accuracy 79.25696\n",
      "loss at epoch 70 was 0.02035\n",
      " Evaluation with training data got 3766 / 3770 with accuracy 99.89390\n",
      " Evaluation at testing data got 1296 / 1615 with accuracy 80.24768\n",
      "loss at epoch 75 was 0.02395\n",
      " Evaluation with training data got 3768 / 3770 with accuracy 99.94695\n",
      " Evaluation at testing data got 1272 / 1615 with accuracy 78.76161\n",
      "loss at epoch 80 was 0.01727\n",
      " Evaluation with training data got 3768 / 3770 with accuracy 99.94695\n",
      " Evaluation at testing data got 1294 / 1615 with accuracy 80.12384\n",
      "loss at epoch 85 was 0.01937\n",
      " Evaluation with training data got 3770 / 3770 with accuracy 100.00000\n",
      " Evaluation at testing data got 1291 / 1615 with accuracy 79.93808\n",
      "loss at epoch 90 was 0.01035\n",
      " Evaluation with training data got 3768 / 3770 with accuracy 99.94695\n",
      " Evaluation at testing data got 1284 / 1615 with accuracy 79.50464\n",
      "loss at epoch 95 was 0.02313\n",
      " Evaluation with training data got 3766 / 3770 with accuracy 99.89390\n",
      " Evaluation at testing data got 1293 / 1615 with accuracy 80.06192\n",
      "loss at epoch 100 was 0.00953\n",
      " Evaluation with training data got 3769 / 3770 with accuracy 99.97348\n",
      " Evaluation at testing data got 1308 / 1615 with accuracy 80.99071\n",
      "loss at epoch 105 was 0.00846\n",
      " Evaluation with training data got 3770 / 3770 with accuracy 100.00000\n",
      " Evaluation at testing data got 1298 / 1615 with accuracy 80.37152\n",
      "loss at epoch 110 was 0.00886\n",
      " Evaluation with training data got 3766 / 3770 with accuracy 99.89390\n",
      " Evaluation at testing data got 1290 / 1615 with accuracy 79.87616\n",
      "loss at epoch 115 was 0.00895\n",
      " Evaluation with training data got 3769 / 3770 with accuracy 99.97348\n",
      " Evaluation at testing data got 1301 / 1615 with accuracy 80.55727\n",
      "loss at epoch 120 was 0.01360\n",
      " Evaluation with training data got 3770 / 3770 with accuracy 100.00000\n",
      " Evaluation at testing data got 1297 / 1615 with accuracy 80.30960\n",
      "loss at epoch 125 was 0.01252\n",
      " Evaluation with training data got 3769 / 3770 with accuracy 99.97348\n",
      " Evaluation at testing data got 1305 / 1615 with accuracy 80.80496\n",
      "loss at epoch 130 was 0.00706\n",
      " Evaluation with training data got 3770 / 3770 with accuracy 100.00000\n",
      " Evaluation at testing data got 1306 / 1615 with accuracy 80.86687\n",
      "loss at epoch 135 was 0.00431\n",
      " Evaluation with training data got 3769 / 3770 with accuracy 99.97348\n",
      " Evaluation at testing data got 1303 / 1615 with accuracy 80.68112\n",
      "loss at epoch 140 was 0.00965\n",
      " Evaluation with training data got 3767 / 3770 with accuracy 99.92042\n",
      " Evaluation at testing data got 1298 / 1615 with accuracy 80.37152\n",
      "loss at epoch 145 was 0.00724\n",
      " Evaluation with training data got 3769 / 3770 with accuracy 99.97348\n",
      " Evaluation at testing data got 1309 / 1615 with accuracy 81.05263\n",
      "loss at epoch 150 was 0.00576\n",
      " Evaluation with training data got 3770 / 3770 with accuracy 100.00000\n",
      " Evaluation at testing data got 1304 / 1615 with accuracy 80.74303\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(151):\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, (data,targets) in enumerate(train_loader):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.flatten().to(device=device)\n",
    "\n",
    "        # forwards\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # backwards\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent\n",
    "        optimizer.step()\n",
    "    \n",
    "    mean_loss = sum(losses)/len(losses)\n",
    "    \n",
    "    if epoch % 5 ==0:\n",
    "        print(f'loss at epoch {epoch} was {mean_loss:.5f}')\n",
    "        num_correct, num_samples  = check_accuracy(loader = train_loader, model=model)\n",
    "        print(f' Evaluation with training data got {num_correct} / {num_samples} with accuracy {float(num_correct/num_samples)*100:.5f}')\n",
    "        num_correct, num_samples  = check_accuracy(loader = test_loader, model=model)\n",
    "        print(f' Evaluation at testing data got {num_correct} / {num_samples} with accuracy {float(num_correct/num_samples)*100:.5f}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 4, 5, 4, 1]) tensor([2, 0, 0, 2, 0])\n",
      "torch.return_types.max(\n",
      "values=tensor([-0.0575,  0.1082,  0.2198,  0.4747,  0.5401], grad_fn=<MaxBackward0>),\n",
      "indices=tensor([1, 4, 5, 4, 1]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GLN(\n",
       "  (LCL0): LocallyConnectedLayer()\n",
       "  (LCLayers): Sequential(\n",
       "    (0): LCBlock(\n",
       "      (bn): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (silu): SiLU()\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (LCL1): LocallyConnectedLayer()\n",
       "      (LCL2): LocallyConnectedLayer()\n",
       "      (identity_downsample): Linear(in_features=500, out_features=125, bias=True)\n",
       "    )\n",
       "    (1): LCBlock(\n",
       "      (bn): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (silu): SiLU()\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (LCL1): LocallyConnectedLayer()\n",
       "      (LCL2): LocallyConnectedLayer()\n",
       "      (identity_downsample): Linear(in_features=125, out_features=32, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (FCLayers): Sequential(\n",
       "    (0): FCBlock(\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (silu): SiLU()\n",
       "      (drop): Dropout(p=0.5, inplace=False)\n",
       "      (FCL1): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (FCL2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (identity_downsample): Linear(in_features=128, out_features=256, bias=True)\n",
       "    )\n",
       "    (1): FCBlock(\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (silu): SiLU()\n",
       "      (drop): Dropout(p=0.5, inplace=False)\n",
       "      (FCL1): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (FCL2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (2): FCBlock(\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (silu): SiLU()\n",
       "      (drop): Dropout(p=0.5, inplace=False)\n",
       "      (FCL1): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (FCL2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (silu): SiLU()\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       "  (Linear): Linear(in_features=256, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "scores = model(test_array)\n",
    "print(torch.argmax(scores, dim=1), test_batch[1].flatten())\n",
    "print(scores.max(1))\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, num = check_accuracy(loader = test_loader, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.399999618530273\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'my_model')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
