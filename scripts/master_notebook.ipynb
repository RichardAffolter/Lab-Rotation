{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sampleDataset(Dataset):\n",
    "    def __init__(self, csv_file, array_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.array_dir = array_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        array_path = os.path.join(self.array_dir, self.annotations.iloc[index, 0]+\".npy\")\n",
    "        array = np.load(array_path)\n",
    "        array = array.astype('float32')\n",
    "        y_label = annotations.iloc[index,1]\n",
    "\n",
    "        if self.transform:\n",
    "            array = self.transform(array)\n",
    "        \n",
    "        return array, y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"/home/richard/labrotation/processed_sample_data/human_origins_labels.csv\"\n",
    "array_path = \"//home/richard/labrotation/processed_sample_data/arrays\"\n",
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_csv(csv_path)\n",
    "array_dir = array_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sampleDataset(csv_file=csv_path, array_dir=array_dir)\n",
    "data_loader = DataLoader(dataset=dataset, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch = next(iter(data_loader))\n",
    "test_array = test_batch[0]\n",
    "test_array.shape\n",
    "test_array.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "expected input: Tensor of dimensions N*C*in_features\n",
    "output: Tensor of dimensions N*out_sets*⌈(in_features/m)⌉\n",
    "N: batch size\n",
    "C: number of channels (e.g. 4 for one-hot-encoding of SNPs)\n",
    "in_features: number of input features (e.g. SNP positions)\n",
    "out_sets: number of output sets (new channels)\n",
    "m: how many in_features to group together\n",
    "kernel_size: kernel of flat tensor: m*C\n",
    "padding: should we padd at the end of the dataframe if in_features%m !=0? \n",
    "'''\n",
    "class LocallyConnectedLayer(torch.nn.Module):\n",
    "    def __init__(self, in_features, m, C=4, padding=True, bias=False, out_sets=4):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.C = C\n",
    "        self.m = m\n",
    "        self.kernel_size = m*C\n",
    "        self.padding = (m-(in_features%m))%m*C if padding else 0\n",
    "        self.weight = nn.Parameter(torch.randn(1,self.kernel_size, out_sets))\n",
    "        self.bias = nn.Parameter(torch.randn(1,out_sets)) if bias else None # with batchnorm we do not need bias\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(-1,-2) # we need to transpose first to ensure that the channel values of one in_feature are next to each other after flattening\n",
    "        x = x.flatten(1) # dim(N,in_features*C)\n",
    "        x = F.pad(x,(0,self.padding))\n",
    "        x = x.unfold(-1, size=self.kernel_size, step=self.kernel_size)\n",
    "        x = torch.matmul(x,self.weight)\n",
    "        if self.bias is not None:\n",
    "            x = x+self.bias\n",
    "        x = x = x.transpose(-1,-2) # transpose back to have the more convenient dimension order\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LCBlock(nn.Module):\n",
    "    def __init__(self, in_features, m, out_sets=4, p=0.0):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm1d(out_sets)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.LCL1 = LocallyConnectedLayer(in_features, m=m, padding=True, out_sets=out_sets)\n",
    "        self.LCL2 = LocallyConnectedLayer(in_features=math.ceil(in_features/m),m=m, padding=True, out_sets=out_sets)\n",
    "        self.identity_downsample = nn.Linear(in_features, out_features=math.ceil(in_features/m**2)) if m!=1 else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        x = self.bn(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.LCL1(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.LCL2(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "\n",
    "        x = x+identity\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "expected input: flat tensor of shape N*in_features\n",
    "expected output: flat tensor of shape N*out_features\n",
    "N: batch size\n",
    "'''\n",
    "class FCBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, p=0.5):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm1d(in_features)\n",
    "        self.bn2 = nn.BatchNorm1d(out_features)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.FCL1 = nn.Linear(in_features=in_features, out_features=out_features)\n",
    "        self.FCL2 = nn.Linear(in_features=out_features, out_features=out_features)\n",
    "        self.identity_downsample = nn.Linear(in_features, out_features=out_features) if in_features != out_features else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        x = self.bn1(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.FCL1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.FCL2(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "\n",
    "        x = x+identity\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLN(nn.Module):\n",
    "    def __init__(self, in_features, num_residual_blocks, m1=2, m2=2):\n",
    "        super().__init__()\n",
    "        self.m1 = m1\n",
    "        self.m2 = m2\n",
    "        self.num_residual_blocks = num_residual_blocks\n",
    "        self.LCL0 = LocallyConnectedLayer(in_features, m=m1)\n",
    "        Output1 = math.ceil(in_features/m1)\n",
    "        self.LCLayers = self.make_LCLayers(Output1)\n",
    "        \n",
    "\n",
    "    def make_LCLayers(self, in_features):\n",
    "        layers = []\n",
    "        \n",
    "        for block in range(self.num_residual_blocks):\n",
    "            layers.append(LCBlock(in_features=in_features, m=self.m2))\n",
    "            in_features = math.ceil(in_features/self.m2**2)\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def make_predictorLayers(in_features):\n",
    "        layers = []\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.LCL0(x)\n",
    "        x = self.LCLayers(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GLN(in_features=1000, num_residual_blocks=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 32])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test_array).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/richard/labrotation/Lab-Rotation/scripts/master_notebook.ipynb Cell 12'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/richard/labrotation/Lab-Rotation/scripts/master_notebook.ipynb#ch0000012vscode-remote?line=0'>1</a>\u001b[0m test_array\u001b[39m.\u001b[39mdtype\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/richard/labrotation/Lab-Rotation/scripts/master_notebook.ipynb#ch0000012vscode-remote?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mforward(test_array\u001b[39m.\u001b[39mflatten(\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mshape\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/richard/labrotation/Lab-Rotation/scripts/master_notebook.ipynb#ch0000012vscode-remote?line=2'>3</a>\u001b[0m bla\u001b[39m.\u001b[39mforward(test_array)\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_array' is not defined"
     ]
    }
   ],
   "source": [
    "test_array.dtype\n",
    "model.forward(test_array.flatten(1)).shape\n",
    "bla.forward(test_array).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "fc0 = LocallyConnectedLayer(in_features=1000,m=2,bias=False)\n",
    "a = torch.from_numpy(array).unsqueeze(0).float()\n",
    "out = fc0.forward(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = LocallyConnectedLayer(in_features=500, m=8).forward(a)\n",
    "c = LocallyConnectedLayer(in_features=125,m=8).forward(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.625"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000/4**3"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
