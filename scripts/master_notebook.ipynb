{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"/home/richard/labrotation/processed_sample_data/human_origins_labels.csv\"\n",
    "array_path = \"//home/richard/labrotation/processed_sample_data/arrays\"\n",
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sampleDataset(Dataset):\n",
    "    def __init__(self, csv_file, array_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.array_dir = array_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        array_path = os.path.join(self.array_dir, self.annotations.iloc[index, 0]+\".npy\")\n",
    "        array = np.load(array_path)\n",
    "        array = array.astype('float32')\n",
    "        y_label = annotations.iloc[index,1]\n",
    "\n",
    "        if self.transform:\n",
    "            y_label = self.transform(y_label)\n",
    "        \n",
    "        return array, y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_csv(csv_path)\n",
    "array_dir = array_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation(x):\n",
    "    x = le.transform((x,))\n",
    "    x = x[0]\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = annotations['Origin'].unique()\n",
    "le = LabelEncoder()\n",
    "le.fit(classes)\n",
    "dataset = sampleDataset(csv_file=csv_path, array_dir=array_dir, transform=transformation)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=5, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=5, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 5, 0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch = next(iter(test_loader))\n",
    "test_array = test_batch[0]\n",
    "test_batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "expected input: Tensor of dimensions N*C*in_features\n",
    "output: Tensor of dimensions N*out_sets*⌈(in_features/m)⌉\n",
    "N: batch size\n",
    "C: number of channels (e.g. 4 for one-hot-encoding of SNPs)\n",
    "in_features: number of input features (e.g. SNP positions)\n",
    "out_sets: number of output sets (new channels)\n",
    "m: how many in_features to group together\n",
    "kernel_size: kernel of flat tensor: m*C\n",
    "padding: should we padd at the end of the dataframe if in_features%m !=0? \n",
    "'''\n",
    "class LocallyConnectedLayer(torch.nn.Module):\n",
    "    def __init__(self, in_features, m, C=4, padding=True, bias=False, out_sets=4):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.C = C\n",
    "        self.m = m\n",
    "        self.kernel_size = m*C\n",
    "        self.padding = (m-(in_features%m))%m*C if padding else 0\n",
    "        self.weight = nn.Parameter(torch.randn(1,self.kernel_size, out_sets))\n",
    "        self.bias = nn.Parameter(torch.randn(1,out_sets)) if bias else None # with batchnorm we do not need bias\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(-1,-2) # we need to transpose first to ensure that the channel values of one in_feature are next to each other after flattening\n",
    "        x = x.flatten(1) # dim(N,in_features*C)\n",
    "        x = F.pad(x,(0,self.padding))\n",
    "        x = x.unfold(-1, size=self.kernel_size, step=self.kernel_size)\n",
    "        x = torch.matmul(x,self.weight)\n",
    "        if self.bias is not None:\n",
    "            x = x+self.bias\n",
    "        x = x = x.transpose(-1,-2) # transpose back to have the more convenient dimension order\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LCBlock(nn.Module):\n",
    "    def __init__(self, in_features, m, out_sets=4, p=0.0):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm1d(out_sets)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.LCL1 = LocallyConnectedLayer(in_features, m=m, padding=True, out_sets=out_sets)\n",
    "        self.LCL2 = LocallyConnectedLayer(in_features=math.ceil(in_features/m),m=m, padding=True, out_sets=out_sets)\n",
    "        self.identity_downsample = nn.Linear(in_features, out_features=math.ceil(in_features/m**2)) if m!=1 else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        x = self.bn(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.LCL1(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.LCL2(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "\n",
    "        x = x+identity\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "expected input: flat tensor of shape N*in_features\n",
    "expected output: flat tensor of shape N*out_features\n",
    "N: batch size\n",
    "'''\n",
    "class FCBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, p=0.5):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm1d(in_features)\n",
    "        self.bn2 = nn.BatchNorm1d(out_features)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.FCL1 = nn.Linear(in_features=in_features, out_features=out_features)\n",
    "        self.FCL2 = nn.Linear(in_features=out_features, out_features=out_features)\n",
    "        self.identity_downsample = nn.Linear(in_features, out_features=out_features) if in_features != out_features else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        x = self.bn1(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.FCL1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.FCL2(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "\n",
    "        x = x+identity\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLN(nn.Module):\n",
    "    def __init__(self, in_features, num_classes, num_residual_blocks=2, m1=2, m2=2, C=4, num_predictor_blocks=4):\n",
    "        super().__init__()\n",
    "        self.m1 = m1\n",
    "        self.m2 = m2\n",
    "        self.num_residual_blocks = num_residual_blocks\n",
    "        self.num_predictor_blocks = num_residual_blocks\n",
    "        self.LCL0 = LocallyConnectedLayer(in_features, m=m1)\n",
    "        Output1 = math.ceil(in_features/m1)\n",
    "        self.LCLayers = self.make_LCLayers(Output1)\n",
    "        Output2 = math.ceil(Output1/(2*m2)**num_residual_blocks)*C # we flatten after the last block TO DO: IMPLEMENT ENVIRONMENT CONCATENATION\n",
    "        self.FCLayers = self.make_predictorLayers(in_features=Output2)\n",
    "        self.bn = nn.BatchNorm1d(256)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        self.Linear = nn.Linear(256,num_classes)\n",
    "        \n",
    "\n",
    "    def make_LCLayers(self, in_features):\n",
    "        layers = []\n",
    "        for block in range(self.num_residual_blocks):\n",
    "            layers.append(LCBlock(in_features=in_features, m=self.m2))\n",
    "            in_features = math.ceil(in_features/self.m2**2)\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def make_predictorLayers(self, in_features):\n",
    "        layers = []\n",
    "        layers.append(FCBlock(in_features=in_features, out_features=256))\n",
    "        for block in range(self.num_predictor_blocks):\n",
    "            layers.append(FCBlock(in_features=256, out_features=256))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.LCL0(x)\n",
    "        x = self.LCLayers(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.FCLayers(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.Linear(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GLN(in_features=1000, num_classes=6,num_residual_blocks=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            scores = model(x)\n",
    "            predictions = torch.argmax(scores, dim=1)\n",
    "            \n",
    "\n",
    "            num_correct += (predictions==y).sum()\n",
    "            num_samples += predictions.shape[0]\n",
    "        #acc = float(num_correct/num_samples)*100\n",
    "        #print(f' Got {num_correct} / {num_samples} with accuracy {float(num_correct/num_samples)*100}')\n",
    "\n",
    "    model.train()\n",
    "    return(num_correct,num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 0 was 1.01107\n",
      " Evaluation with training data got 3174 / 3770 with accuracy 84.19098\n",
      " Evaluation at testing data got 1247 / 1615 with accuracy 77.21362\n",
      "loss at epoch 5 was 0.24481\n",
      " Evaluation with training data got 3697 / 3770 with accuracy 98.06366\n",
      " Evaluation at testing data got 1303 / 1615 with accuracy 80.68112\n",
      "loss at epoch 10 was 0.12299\n",
      " Evaluation with training data got 3755 / 3770 with accuracy 99.60212\n",
      " Evaluation at testing data got 1296 / 1615 with accuracy 80.24768\n",
      "loss at epoch 15 was 0.08525\n",
      " Evaluation with training data got 3761 / 3770 with accuracy 99.76127\n",
      " Evaluation at testing data got 1295 / 1615 with accuracy 80.18576\n",
      "loss at epoch 20 was 0.06532\n",
      " Evaluation with training data got 3752 / 3770 with accuracy 99.52255\n",
      " Evaluation at testing data got 1310 / 1615 with accuracy 81.11455\n",
      "loss at epoch 25 was 0.05530\n",
      " Evaluation with training data got 3750 / 3770 with accuracy 99.46949\n",
      " Evaluation at testing data got 1296 / 1615 with accuracy 80.24768\n",
      "loss at epoch 30 was 0.05188\n",
      " Evaluation with training data got 3762 / 3770 with accuracy 99.78780\n",
      " Evaluation at testing data got 1316 / 1615 with accuracy 81.48607\n",
      "loss at epoch 35 was 0.03334\n",
      " Evaluation with training data got 3761 / 3770 with accuracy 99.76127\n",
      " Evaluation at testing data got 1307 / 1615 with accuracy 80.92879\n",
      "loss at epoch 40 was 0.04564\n",
      " Evaluation with training data got 3764 / 3770 with accuracy 99.84085\n",
      " Evaluation at testing data got 1306 / 1615 with accuracy 80.86687\n",
      "loss at epoch 45 was 0.04062\n",
      " Evaluation with training data got 3759 / 3770 with accuracy 99.70822\n",
      " Evaluation at testing data got 1307 / 1615 with accuracy 80.92879\n",
      "loss at epoch 50 was 0.02810\n",
      " Evaluation with training data got 3767 / 3770 with accuracy 99.92042\n",
      " Evaluation at testing data got 1326 / 1615 with accuracy 82.10526\n",
      "loss at epoch 55 was 0.03009\n",
      " Evaluation with training data got 3767 / 3770 with accuracy 99.92042\n",
      " Evaluation at testing data got 1323 / 1615 with accuracy 81.91950\n",
      "loss at epoch 60 was 0.03654\n",
      " Evaluation with training data got 3768 / 3770 with accuracy 99.94695\n",
      " Evaluation at testing data got 1332 / 1615 with accuracy 82.47678\n",
      "loss at epoch 65 was 0.02075\n",
      " Evaluation with training data got 3766 / 3770 with accuracy 99.89390\n",
      " Evaluation at testing data got 1321 / 1615 with accuracy 81.79567\n",
      "loss at epoch 70 was 0.02836\n",
      " Evaluation with training data got 3767 / 3770 with accuracy 99.92042\n",
      " Evaluation at testing data got 1311 / 1615 with accuracy 81.17647\n",
      "loss at epoch 75 was 0.02730\n",
      " Evaluation with training data got 3767 / 3770 with accuracy 99.92042\n",
      " Evaluation at testing data got 1311 / 1615 with accuracy 81.17647\n",
      "loss at epoch 80 was 0.01673\n",
      " Evaluation with training data got 3767 / 3770 with accuracy 99.92042\n",
      " Evaluation at testing data got 1314 / 1615 with accuracy 81.36223\n",
      "loss at epoch 85 was 0.01569\n",
      " Evaluation with training data got 3769 / 3770 with accuracy 99.97348\n",
      " Evaluation at testing data got 1313 / 1615 with accuracy 81.30031\n",
      "loss at epoch 90 was 0.01302\n",
      " Evaluation with training data got 3767 / 3770 with accuracy 99.92042\n",
      " Evaluation at testing data got 1316 / 1615 with accuracy 81.48607\n",
      "loss at epoch 95 was 0.02934\n",
      " Evaluation with training data got 3764 / 3770 with accuracy 99.84085\n",
      " Evaluation at testing data got 1310 / 1615 with accuracy 81.11455\n",
      "loss at epoch 100 was 0.00911\n",
      " Evaluation with training data got 3768 / 3770 with accuracy 99.94695\n",
      " Evaluation at testing data got 1316 / 1615 with accuracy 81.48607\n",
      "loss at epoch 105 was 0.02019\n",
      " Evaluation with training data got 3762 / 3770 with accuracy 99.78780\n",
      " Evaluation at testing data got 1322 / 1615 with accuracy 81.85759\n",
      "loss at epoch 110 was 0.03776\n",
      " Evaluation with training data got 3765 / 3770 with accuracy 99.86737\n",
      " Evaluation at testing data got 1299 / 1615 with accuracy 80.43343\n",
      "loss at epoch 115 was 0.01864\n",
      " Evaluation with training data got 3767 / 3770 with accuracy 99.92042\n",
      " Evaluation at testing data got 1325 / 1615 with accuracy 82.04334\n",
      "loss at epoch 120 was 0.01414\n",
      " Evaluation with training data got 3768 / 3770 with accuracy 99.94695\n",
      " Evaluation at testing data got 1307 / 1615 with accuracy 80.92879\n",
      "loss at epoch 125 was 0.01826\n",
      " Evaluation with training data got 3767 / 3770 with accuracy 99.92042\n",
      " Evaluation at testing data got 1310 / 1615 with accuracy 81.11455\n",
      "loss at epoch 130 was 0.01325\n",
      " Evaluation with training data got 3766 / 3770 with accuracy 99.89390\n",
      " Evaluation at testing data got 1310 / 1615 with accuracy 81.11455\n",
      "loss at epoch 135 was 0.01065\n",
      " Evaluation with training data got 3768 / 3770 with accuracy 99.94695\n",
      " Evaluation at testing data got 1312 / 1615 with accuracy 81.23839\n",
      "loss at epoch 140 was 0.01768\n",
      " Evaluation with training data got 3767 / 3770 with accuracy 99.92042\n",
      " Evaluation at testing data got 1315 / 1615 with accuracy 81.42415\n",
      "loss at epoch 145 was 0.01440\n",
      " Evaluation with training data got 3770 / 3770 with accuracy 100.00000\n",
      " Evaluation at testing data got 1334 / 1615 with accuracy 82.60062\n",
      "loss at epoch 150 was 0.01499\n",
      " Evaluation with training data got 3766 / 3770 with accuracy 99.89390\n",
      " Evaluation at testing data got 1315 / 1615 with accuracy 81.42415\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(151):\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, (data,targets) in enumerate(train_loader):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.flatten().to(device=device)\n",
    "\n",
    "        # forwards\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # backwards\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent\n",
    "        optimizer.step()\n",
    "    \n",
    "    mean_loss = sum(losses)/len(losses)\n",
    "    \n",
    "    if epoch % 5 ==0:\n",
    "        print(f'loss at epoch {epoch} was {mean_loss:.5f}')\n",
    "        num_correct, num_samples  = check_accuracy(loader = train_loader, model=model)\n",
    "        print(f' Evaluation with training data got {num_correct} / {num_samples} with accuracy {float(num_correct/num_samples)*100:.5f}')\n",
    "        num_correct, num_samples  = check_accuracy(loader = test_loader, model=model)\n",
    "        print(f' Evaluation at testing data got {num_correct} / {num_samples} with accuracy {float(num_correct/num_samples)*100:.5f}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, num = check_accuracy(loader = test_loader, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.399999618530273\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'my_model')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
